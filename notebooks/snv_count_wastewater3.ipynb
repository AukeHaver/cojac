{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count snvs in wastewater samples from shorah output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import csv\n",
    "from Bio import SeqIO\n",
    "import subprocess\n",
    "from IPython.core.display import display, HTML\n",
    "from termcolor import colored\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import pysam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_range(filename):\n",
    "    '''extract the window range from a shorah window filename:'''\n",
    "    match = re.search('([0-9]+)\\-([0-9]+).reads', filename)\n",
    "    return (int(match.group(1)), int(match.group(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_snvs(filename, shorah_table):\n",
    "    '''Function to produce a n_local_haplot X p_snv_falling_in_the_local_haplo_window table of snv counts\n",
    "    Parameters:\n",
    "        filename: str of the name of the fasta.gz file for the shorah window\n",
    "        shorah_table: table outputted by shorah containing positions and \n",
    "    Return:\n",
    "        df_out: pd.DataFrame of snv counts with local haplos in the rows and snv's in the columns\n",
    "    '''\n",
    "    # extract range of window from filename\n",
    "    seqstart, seqstop = extract_range(filename)\n",
    "    # subset rows of shorah table for snv's falling in that range\n",
    "    shorah_table_subset = shorah_table[(shorah_table[\"position\"] >= seqstart) & (shorah_table[\"position\"] <= seqstop)]\n",
    "    # stop there and return None if no snv's fall in that range\n",
    "    if shorah_table_subset.shape[0] == 0:\n",
    "        return None\n",
    "    else:\n",
    "        with gzip.open(filename, 'rt') as f:\n",
    "            window_lst = [] \n",
    "            window_names = []\n",
    "            # iterate through local haplos \n",
    "            for record in SeqIO.parse(f, \"fasta\"):\n",
    "                # keep seq name\n",
    "                window_names.append(record.description)\n",
    "                snv_lst = []\n",
    "                # iterate through snvs falling in the window\n",
    "                for i in range(shorah_table_subset.shape[0]):\n",
    "                    # test if the snv is present in this local haplo\n",
    "                    snv_lst.append((record.seq[shorah_table_subset[\"position\"].values[i]-seqstart] == \n",
    "                                    shorah_table_subset[\"variant\"].values[i]))\n",
    "                window_lst.append(snv_lst)    \n",
    "                \n",
    "        haplos_array = np.array(window_lst) * 1\n",
    "        snv_names = shorah_table_subset[\"reference\"] + \\\n",
    "            shorah_table_subset[\"position\"].astype('str') + \\\n",
    "            shorah_table_subset[\"variant\"]\n",
    "        df_out = pd.DataFrame(haplos_array, columns=snv_names, index=window_names)\n",
    "            \n",
    "        return df_out   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define mutations to look for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxmut=re.compile('^(?P<reference>[NATGC])(?P<position>[0-9]+)(?P<variant>[ATGC-])$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_varlist = [\n",
    "\"C3267T\",\n",
    "\"C5388A\",\n",
    "\"T6954C\",\n",
    "\"N11288-\",\n",
    "\"N11289-\",\n",
    "\"N11290-\",\n",
    "\"N11291-\",\n",
    "\"N11292-\",\n",
    "\"N11293-\",\n",
    "\"N11294-\",\n",
    "\"N11295-\",\n",
    "\"N11296-\",\n",
    "\"N21765-\",\n",
    "\"N21766-\",\n",
    "\"N21767-\",\n",
    "\"N21768-\",\n",
    "\"N21769-\",\n",
    "\"N21770-\",\n",
    "\"N21991-\",\n",
    "\"N21992-\",\n",
    "\"N21993-\",\n",
    "\"A23063T\",\n",
    "\"C23271A\",\n",
    "\"C23604A\",\n",
    "\"C23709T\",\n",
    "\"T24506G\",\n",
    "\"G24914C\",\n",
    "\"C27972T\",\n",
    "\"G28048T\",\n",
    "\"A28111G\",\n",
    "#\"28280 GAT->CTA\",\n",
    "\"G28280C\",\n",
    "\"A28281T\",\n",
    "\"T28282A\",\n",
    "\"C28977T\"]\n",
    "UK_vartable = pd.DataFrame(data=[rxmut.match(i).groupdict() for i in UK_varlist])\n",
    "UK_vartable[\"position\"] = UK_vartable[\"position\"].astype('int')\n",
    "UK_vartable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA_varlist = [\n",
    "\"C1059T\",\n",
    "\"G5230T\",\n",
    "\"A10323G\",\n",
    "\"A21801C\",\n",
    "\"G22813T\",\n",
    "\"G23012A\",\n",
    "\"A23063T\",\n",
    "\"C23664T\",\n",
    "\"G25563T\",\n",
    "\"C25904T\",\n",
    "\"C26456T\",\n",
    "\"C28887T\"]\n",
    "SA_vartable = pd.DataFrame(data=[rxmut.match(i).groupdict() for i in SA_varlist])\n",
    "SA_vartable[\"position\"] = SA_vartable[\"position\"].astype('int')\n",
    "SA_vartable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all wastewater samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t='ww.tsv'\n",
    "with open(t,'rt',encoding='utf-8') as tf:\t# this file has the same content as the original experiment\n",
    "    ww_sampledirs = [f\"working/samples/{r['sample']}/{r['batch']}/variants/SNVs/REGION_1/support/\" for r in csv.DictReader(tf, dialect='excel-tab')]\n",
    "ww_sampledirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dirlist = [ww_sampledirs[0] + i for i in os.listdir(ww_sampledirs[0])]\n",
    "temp_dirlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    UK_vartable[\"helo\"]\n",
    "except KeyError:\n",
    "    print(\"NO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do it for one mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_posterior = 0.8\n",
    "mut_number = 0\n",
    "temp_dirlist = [ww_sampledirs[0] + i for i in os.listdir(ww_sampledirs[0])]\n",
    "vartable = UK_vartable\n",
    "\n",
    "# find all snv tables for one mutation\n",
    "tmp_snvcounts = []\n",
    "mut_name = vartable.iloc[mut_number][\"reference\"] + \\\n",
    "    str(vartable.iloc[mut_number][\"position\"]) + \\\n",
    "    vartable.iloc[mut_number][\"variant\"]\n",
    "candidate_windows = 0 # keep track of candidate windows\n",
    "for win in temp_dirlist:\n",
    "    strt, stp = extract_range(win)\n",
    "    if strt <= vartable[\"position\"][mut_number] <= stp:\n",
    "        candidate_windows += 1\n",
    "        try:\n",
    "            snv_tab = count_snvs(win, vartable)[mut_name]\n",
    "        except KeyError:\n",
    "            snv_tab = None\n",
    "        if snv_tab is not None:\n",
    "            tmp_snvcounts.append(snv_tab)\n",
    "\n",
    "# sum haplos in each window and take the average\n",
    "ave_reads_full_lst = []\n",
    "for win in range(len(tmp_snvcounts)):\n",
    "    ave_reads_lst = []\n",
    "    for haplo in range(tmp_snvcounts[win].shape[0]):\n",
    "        haplo_name = tmp_snvcounts[win].index[haplo]\n",
    "        posterior = float(re.search(\"posterior=([0-1][\\.]{0,1}[0-9]{0,})\", haplo_name).group(1))\n",
    "        ave_reads = float(re.search(\"ave_reads=([0-9]+[\\.]{0,1}[0-9]{0,})\", haplo_name).group(1))\n",
    "        if posterior > min_posterior:\n",
    "            if tmp_snvcounts[win][haplo] == 1:\n",
    "                ave_reads_lst.append(ave_reads)\n",
    "    ave_reads_tmp = sum(ave_reads_lst)\n",
    "    ave_reads_full_lst.append(ave_reads_tmp)\n",
    "effective_windows = len(ave_reads_full_lst)\n",
    "ave_r = np.average(ave_reads_full_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_one_mut(temp_dirlist, vartable, mut_number, min_posterior=0.9):\n",
    "    '''Look for mutation number (mut_number) of (vartable) in (temp_dirlist)'''\n",
    "    # find all snv tables for one mutation\n",
    "    tmp_snvcounts = []\n",
    "    mut_name = vartable.iloc[mut_number][\"reference\"] + \\\n",
    "        str(vartable.iloc[mut_number][\"position\"]) + \\\n",
    "        vartable.iloc[mut_number][\"variant\"]\n",
    "    candidate_windows = 0 # keep track of candidate windows\n",
    "    for win in temp_dirlist:\n",
    "        strt, stp = extract_range(win)\n",
    "        if strt <= vartable[\"position\"][mut_number] <= stp:\n",
    "            candidate_windows += 1\n",
    "            try:\n",
    "                snv_tab = count_snvs(win, vartable)[mut_name]\n",
    "            except KeyError:\n",
    "                snv_tab = None\n",
    "            if snv_tab is not None:\n",
    "                tmp_snvcounts.append(snv_tab)\n",
    "\n",
    "    # sum haplos in each window and take the average\n",
    "    ave_reads_full_lst = []\n",
    "    for win in range(len(tmp_snvcounts)):\n",
    "        ave_reads_lst = []\n",
    "        for haplo in range(tmp_snvcounts[win].shape[0]):\n",
    "            haplo_name = tmp_snvcounts[win].index[haplo]\n",
    "            posterior = float(re.search(\"posterior=([0-1][\\.]{0,1}[0-9]{0,})\", haplo_name).group(1))\n",
    "            ave_reads = float(re.search(\"ave_reads=([0-9]+[\\.]{0,1}[0-9]{0,})\", haplo_name).group(1))\n",
    "            if posterior > min_posterior:\n",
    "                if tmp_snvcounts[win][haplo] == 1:\n",
    "                    ave_reads_lst.append(ave_reads)\n",
    "        ave_reads_tmp = sum(ave_reads_lst)\n",
    "        ave_reads_full_lst.append(ave_reads_tmp)\n",
    "    effective_windows = sum([i>0 for i in ave_reads_full_lst])\n",
    "\n",
    "    # compute average \n",
    "    ave_r = np.average(ave_reads_full_lst) if len(ave_reads_full_lst) else 0\n",
    "    if not len(ave_reads_full_lst):\n",
    "        warnname=os.sep.join(str(temp_dirlist[0]).split(os.sep)[:-2])\n",
    "        print(f\"Warning! Can't average in {warnname}\")\n",
    "\n",
    "    return (candidate_windows, effective_windows, ave_r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all_mut(temp_dirlist, vartable, min_posterior=0.9):\n",
    "    arr1 = np.array([list(search_one_mut(temp_dirlist, vartable, i, min_posterior)) for i in range(vartable.shape[0])])\n",
    "    temp_df = pd.DataFrame(arr1, columns=[\"candidate_windows\", \"effective_windows\", \"ave_reads\"])\n",
    "    temp_df = pd.concat([vartable, temp_df], axis=1)\n",
    "    return temp_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make all UK outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_UK_dfs = []\n",
    "\n",
    "for sample in tqdm(ww_sampledirs):\n",
    "    # check if ShoRAH did output windows there\n",
    "    if not os.path.isdir(sample):\n",
    "        print(f\"Warning! No windows in {sample}!!!\")\n",
    "        continue\n",
    "\n",
    "    temp_dirlist = [sample + i for i in os.listdir(sample)]\n",
    "    mut_df_UK = search_all_mut(temp_dirlist, UK_vartable, min_posterior=0.9)\n",
    "    all_UK_dfs.append(mut_df_UK)\n",
    "    spl=sample.split(os.sep)\n",
    "    mut_df_UK.to_csv(os.path.join('uk_snv_tables', f\"{spl[2]}-{spl[3]}_uk_snv.csv\"), na_rep=\"NA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all SA outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_SA_dfs = []\n",
    "\n",
    "for sample in tqdm(ww_sampledirs):\n",
    "    # check if ShoRAH did output windows there\n",
    "    if not os.path.isdir(sample):\n",
    "        print(f\"Warning! No windows in {sample}!!!\")\n",
    "        continue\n",
    "\n",
    "    temp_dirlist = [sample + i for i in os.listdir(sample)]\n",
    "    mut_df_SA = search_all_mut(temp_dirlist, SA_vartable, min_posterior=0.9)\n",
    "    all_SA_dfs.append(mut_df_SA)\n",
    "    spl=sample.split(os.sep)\n",
    "    mut_df_SA.to_csv(os.path.join('sa_snv_tables/', f\"{spl[2]}-{spl[3]}_sa_snv.csv\"), na_rep=\"NA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double checking code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_sampledirs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dfnum in range(len(all_UK_dfs)):\n",
    "    target_dir = ww_sampledirs[dfnum]\n",
    "    for i in range(all_UK_dfs[dfnum].shape[0]):\n",
    "        if pd.isna(all_UK_dfs[dfnum][\"ave_reads\"][i]):\n",
    "            pos_to_check = all_UK_dfs[dfnum][\"position\"][i]\n",
    "            print(pos_to_check)\n",
    "            lst1 = [ww_sampledirs[dfnum] + i for i in os.listdir(ww_sampledirs[dfnum])]\n",
    "            for d in lst1:\n",
    "                strt, stop = extract_range(d)\n",
    "                if strt <= pos_to_check <= stop:\n",
    "                    print(\"PROBLEM\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dfnum in range(len(all_SA_dfs)):\n",
    "    target_dir = ww_sampledirs[dfnum]\n",
    "    for i in range(all_SA_dfs[dfnum].shape[0]):\n",
    "        if pd.isna(all_SA_dfs[dfnum][\"ave_reads\"][i]):\n",
    "            pos_to_check = all_SA_dfs[dfnum][\"position\"][i]\n",
    "            print(pos_to_check)\n",
    "            lst1 = [ww_sampledirs[dfnum] + i for i in os.listdir(ww_sampledirs[dfnum])]\n",
    "            for d in lst1:\n",
    "                strt, stop = extract_range(d)\n",
    "                if strt <= pos_to_check <= stop:\n",
    "                    print(\"PROBLEM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_varlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio1",
   "language": "python",
   "name": "bio1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
